{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.cm as cm\n",
    "import pylab\n",
    "import math\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.fftpack import rfft, irfft, fftfreq, fft, ifft, fftshift\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChanName.mat\n",
    "\n",
    "通道的名称\n",
    "\n",
    "一共24个通道，rawTracePersonX之中只包含19个通道[1:8 10:16 19:20 23:24]，不包含9CM 17X3 18X2 21X1 22A2。cm（废弃通道），X1 X2 X3 A2为空通道（什么都没接）。\n",
    "\n",
    "<img src=\"./source/pic1.png\" width = \"30%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EEG P3 - Pz', 'EEG C3 - Pz', 'EEG F3 - Pz', 'EEG Fz - Pz',\n",
       "       'EEG F4 - Pz', 'EEG C4 - Pz', 'EEG P4 - Pz', 'EEG Cz - Pz',\n",
       "       'EEG CM - Pz', 'EEG A1 - Pz', 'EEG Fp1 - Pz', 'EEG Fp2 - Pz',\n",
       "       'EEG T3 - Pz', 'EEG T5 - Pz', 'EEG O1 - Pz', 'EEG O2 - Pz',\n",
       "       'EEG X3 - Pz', 'EEG X2 - Pz', 'EEG F7 - Pz', 'EEG F8 - Pz',\n",
       "       'EEG X1 - Pz', 'EEG A2 - Pz', 'EEG T6 - Pz', 'EEG T4 - Pz'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChanName = loadmat('./input/ChanName.mat')\n",
    "chanName = np.array(ChanName['ChanName'])\n",
    "chanNamex = chanName.reshape(24,)\n",
    "\n",
    "chanName = []\n",
    "for i in range(24):\n",
    "    chanName.append(chanNamex[i][0][0])\n",
    "chanName = np.array(chanName)\n",
    "chanName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通道：一共24个通道，此处只包含19个通道[1:8 10:16 19:20 23:24]\n",
    "# 不包含9CM 17X3 18X2 21X1 22A2。cm（废弃通道）\n",
    "# X1 X2 X3 A2为空通道（什么都没接）\n",
    "\n",
    "chanNameUsed = np.append(chanName[0:8], chanName[9:16])\n",
    "chanNameUsed = np.append(chanNameUsed, chanName[18:20])\n",
    "chanNameUsed = np.append(chanNameUsed, chanName[22:24])\n",
    "\n",
    "chanNameUsed[11] = 'EEG T7 - Pz'  ## T3 is now T7\n",
    "chanNameUsed[12] = 'EEG P7 - Pz'  ## T5 is now P7\n",
    "chanNameUsed[-2] = 'EEG P8 - Pz'  ## T6 is now P8\n",
    "chanNameUsed[-1] = 'EEG T8 - Pz'  ## T4 is now T8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## raw data\n",
    "\n",
    "具体时间在timeRawTrace.mat（1x2100）文件中。采样频率300Hz 2100个点共对应7s。每个人的timeRawTrace都是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeRawTrace = loadmat('./input/timeRawTrace.mat')\n",
    "timeRawTrace = np.array(timeRawTrace['timeRawTrace'])\n",
    "timeRawTrace = timeRawTrace.reshape(2100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSPerson1 = loadmat('./input/Person1/OSPerson1.mat')\n",
    "\n",
    "os1 = OSPerson1['OS'] # OS：要分析的数据，36x52x40x54 time X freq X Trial x pair\n",
    "time1 = OSPerson1['Time'] # why only 36?\n",
    "freq1 = OSPerson1['fOS'] # fOS：对应的频率\n",
    "track1 = OSPerson1['Track'] # true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 10],\n",
       "       [ 1, 10],\n",
       "       [ 2, 10],\n",
       "       [ 3, 10],\n",
       "       [ 4, 10],\n",
       "       [ 5, 10],\n",
       "       [ 6, 10],\n",
       "       [ 7, 10],\n",
       "       [ 8, 10],\n",
       "       [ 9, 10],\n",
       "       [10, 11],\n",
       "       [10, 12],\n",
       "       [10, 13],\n",
       "       [10, 14],\n",
       "       [10, 15],\n",
       "       [10, 16],\n",
       "       [10, 17],\n",
       "       [10, 18],\n",
       "       [ 0, 18],\n",
       "       [ 1, 18],\n",
       "       [ 2, 18],\n",
       "       [ 3, 18],\n",
       "       [ 4, 18],\n",
       "       [ 5, 18],\n",
       "       [ 6, 18],\n",
       "       [ 7, 18],\n",
       "       [ 8, 18],\n",
       "       [ 9, 18],\n",
       "       [10, 18],\n",
       "       [11, 18],\n",
       "       [12, 18],\n",
       "       [13, 18],\n",
       "       [14, 18],\n",
       "       [15, 18],\n",
       "       [16, 18],\n",
       "       [17, 18],\n",
       "       [ 0,  6],\n",
       "       [ 1,  6],\n",
       "       [ 2,  6],\n",
       "       [ 3,  6],\n",
       "       [ 4,  6],\n",
       "       [ 5,  6],\n",
       "       [ 6,  7],\n",
       "       [ 6,  8],\n",
       "       [ 6,  9],\n",
       "       [ 6, 10],\n",
       "       [ 6, 11],\n",
       "       [ 6, 12],\n",
       "       [ 6, 13],\n",
       "       [ 6, 14],\n",
       "       [ 6, 15],\n",
       "       [ 6, 16],\n",
       "       [ 6, 17],\n",
       "       [ 6, 18]], dtype=uint8)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair54: 54个感兴趣的两个通道之间的配对\n",
    "# 通道：一共24个通道，此处只包含19个通道\n",
    "# [1:8 10:16 19:20 23:24]\n",
    "# [1:8 9:15  16:17 18:19]\n",
    "pair54 = np.array(loadmat('./input/Pair54.mat')['Pair54'])\n",
    "for i in range(54):\n",
    "    for j in range(2):\n",
    "        if 10 <= pair54[i][j] <= 16:# & pair54[i][j] <= 16:\n",
    "            pair54[i][j] -= 2\n",
    "        elif 19 <= pair54[i][j] <= 20:\n",
    "            pair54[i][j] -= 4\n",
    "        elif 23 <= pair54[i][j] <= 24:\n",
    "            pair54[i][j] -= 6\n",
    "        else:\n",
    "            pair54[i][j] -= 1\n",
    "    #pass\n",
    "pair54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawTracePersonX\n",
    "# dataTrail: 2100x40x19 时间 x Trial x 通道，包含了各通道各 Trail 的 Rawtrace\n",
    "# track: 40个trial对应的图片编号。<11的编号为记忆过的图片，>10的是没有记忆过的。顺序和 trail 对应\n",
    "rawTracePerson1 = loadmat('./input/Person1/rawTracePerson1.mat')\n",
    "track1 = np.array(rawTracePerson1['Track']).reshape(40,)\n",
    "dataTrial1 = np.array(rawTracePerson1['dataTrial'])\n",
    "\n",
    "rawTracePerson2 = loadmat('./input/Person2/rawTracePerson2.mat')\n",
    "track2 = np.array(rawTracePerson2['Track']).reshape(10,)\n",
    "dataTrial2 = np.array(rawTracePerson2['dataTrial'])\n",
    "\n",
    "rawTracePerson3 = loadmat('./input/Person3/rawTracePerson3.mat')\n",
    "track3 = np.array(rawTracePerson3['Track']).reshape(40,)\n",
    "dataTrial3 = np.array(rawTracePerson3['dataTrial'])\n",
    "\n",
    "rawTracePerson4 = loadmat('./input/Person4/rawTracePerson4.mat')\n",
    "track4 = np.array(rawTracePerson4['Track']).reshape(10,)\n",
    "dataTrial4 = np.array(rawTracePerson4['dataTrial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_time, filter_freq, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_time(data,i):\n",
    "    '''\n",
    "    filter and output time domain y\n",
    "\n",
    "    data: dataTrial1[:,n_trail,:] # (2100,19) one trail's brain wave\n",
    "    i: data[:,i] # channel we want to see\n",
    "    '''\n",
    "    f = np.zeros((2100, 2), dtype=float)\n",
    "    f[:,1] = data[:,i]\n",
    "    f[:,0] = np.arange(2100)/300\n",
    "\n",
    "    N = 2100 \n",
    "    T = 7.0 / 2100.0\n",
    "    x = f[:,0]\n",
    "    y = f[:,1]\n",
    "\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "\n",
    "    f_signal = rfft(y)\n",
    "    W = fftfreq(y.size, d=x[1]-x[0])\n",
    "\n",
    "    cut_f_signal = f_signal.copy()\n",
    "    cut_f_signal[(W<1)] = 0 # filter all frequencies below 1\n",
    "\n",
    "    cut_signal = irfft(cut_f_signal)\n",
    "    \n",
    "    return cut_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_freq(data,i):\n",
    "    '''\n",
    "    filter and output freq domain y\n",
    "\n",
    "    data: dataTrial1[:,n_trail,:] # (2100,19) one trail's brain wave\n",
    "    i: data[:,i] # channel we want to see\n",
    "    '''\n",
    "    \n",
    "    f = np.zeros((2100, 2), dtype=float)\n",
    "    f[:,1] = data[:,i]\n",
    "    f[:,0] = np.arange(2100)/300\n",
    "\n",
    "    N = 2100 \n",
    "    T = 7.0 / 2100.0 \n",
    "    x = f[:,0]\n",
    "    y = f[:,1]\n",
    "\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "\n",
    "    f_signal = rfft(y)\n",
    "    W = fftfreq(y.size, d=x[1]-x[0])\n",
    "\n",
    "    cut_f_signal = f_signal.copy()\n",
    "    cut_f_signal[(W<1)] = 0  # filter all frequencies below 1\n",
    "\n",
    "    cut_signal = irfft(cut_f_signal)\n",
    "\n",
    "    cut_signal_f = fft(cut_signal)\n",
    "\n",
    "    return cut_signal_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot_freq: plot_freq(track1_remember[1])\n",
    "def plot_freq(n_trail): \n",
    "    '''\n",
    "    randomly choose from 40 trails\n",
    "    plot frequency domain of 19 channel\n",
    "    '''\n",
    "    size = 5\n",
    "    fig = plt.figure(figsize=(size,2*size))\n",
    "    data = dataTrial1[:,n_trail,:] # (2100,19)\n",
    "    data = np.delete(data,7,1) # delete one row\n",
    "    n_rows = 18\n",
    "    n_samples = 2100\n",
    "\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_xlim(0, 5) # x's range: 0-7s\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    dmin = 0\n",
    "    dmax = 20\n",
    "    dr = (dmax - dmin)\n",
    "    y0 = dmin\n",
    "    y1 = (n_rows - 1) * dr + dmax\n",
    "    ax.set_ylim(y0, y1)\n",
    "\n",
    "    N = 2100 \n",
    "    T = 7.0 / 2100.0\n",
    "    tf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    \n",
    "    segs = [] # list of lines, each line is an array of points\n",
    "    for i in range(n_rows):\n",
    "        yf = 2.0/N * np.abs(filter_freq(data,i)[:N//2])\n",
    "        segs.append(np.column_stack((tf, yf)))  ### filter\n",
    "\n",
    "    offsets = np.zeros((n_rows, 2), dtype=float)\n",
    "    offsets[:, 1] = np.arange(n_rows) * dr\n",
    "\n",
    "    lines = LineCollection(segs, offsets=offsets, transOffset=None) # draw lines from segs\n",
    "    ax.add_collection(lines)\n",
    "\n",
    "    ax.set_yticks(np.arange(n_rows) * dr)\n",
    "    ax.set_yticklabels(chanNameUsed)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# plot_time: plot_freq(track1_not_remember[1])\n",
    "def plot_time(n_trail): \n",
    "    '''\n",
    "    randomly choose from 40 trails\n",
    "    plot time domain of 19 channel\n",
    "    '''\n",
    "    \n",
    "    size = 10\n",
    "    fig = plt.figure(figsize=(2*size,size))\n",
    "    n_rows = 18 # delete one row\n",
    "    n_samples = 2100\n",
    "    data = dataTrial1[:,n_trail,:] # (2100,19)\n",
    "    data = np.delete(data,7,1)\n",
    "    t = np.arange(2100) / 300\n",
    "\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_xlim(0, 7.01) # x's range: 0-7s\n",
    "    ax.set_xticks(np.arange(7))\n",
    "    dmin = data.min()\n",
    "    dmax = data.max()\n",
    "    dr = (dmax - dmin)*0.2\n",
    "    y0 = dmin\n",
    "    y1 = (n_rows - 1) * dr + dmax\n",
    "    ax.set_ylim(y0, y1)\n",
    "\n",
    "    segs = [] # list of lines, each line is an array of points\n",
    "    for i in range(n_rows):\n",
    "        segs.append(np.column_stack((t, filter_time(data,i))))  ### filter\n",
    "\n",
    "    offsets = np.zeros((n_rows, 2), dtype=float)\n",
    "    offsets[:, 1] = np.arange(n_rows) * dr\n",
    "\n",
    "    lines = LineCollection(segs, offsets=offsets, transOffset=None) # draw lines from segs\n",
    "    ax.add_collection(lines)\n",
    "\n",
    "    ax.set_yticks(np.arange(n_rows) * dr)\n",
    "    ax.set_yticklabels(chanNameUsed)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.zeros([40+10+40+10])\n",
    "for i in range(40+10+40+10):\n",
    "    if i < 40:\n",
    "        if track1[i] < 11: label[i] = 0\n",
    "        else: label[i] = 1\n",
    "    if 39 < i < 50:\n",
    "        if track2[i-40] < 11: label[i] = 0\n",
    "        else: label[i] = 1\n",
    "    if 49 < i < 90:\n",
    "        if track3[i-50] < 11: label[i] = 0\n",
    "        else: label[i] = 1\n",
    "    if 89 < i < 100:\n",
    "        if track4[i-90] < 11: label[i] = 0\n",
    "        else: label[i] = 1\n",
    "\n",
    "label = np.zeros([40])\n",
    "for i in range(40):\n",
    "    if track1[i] < 11: label[i] = 0\n",
    "    else: label[i] = 1\n",
    "\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 2100)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair\n",
    "ch1 = 1 - 1\n",
    "ch2 = 11 - 1\n",
    "pair_time = np.zeros([40+10+40+10,2100])\n",
    "\n",
    "for i in range(40+10+40+10):\n",
    "    if i < 40:\n",
    "        data = dataTrial1[:,i,:]\n",
    "        pair_time[i] = filter_time(data,ch1) - filter_time(data,ch2)\n",
    "    if 39 < i < 50:\n",
    "        data = dataTrial2[:,i-40,:]\n",
    "        pair_time[i] = filter_time(data,ch1) - filter_time(data,ch2) \n",
    "    if 49 < i < 90:\n",
    "        data = dataTrial3[:,i-50,:]\n",
    "        pair_time[i] = filter_time(data,ch1) - filter_time(data,ch2) \n",
    "    if 89 < i < 100:\n",
    "        data = dataTrial4[:,i-90,:]\n",
    "        pair_time[i] = filter_time(data,ch1) - filter_time(data,ch2)\n",
    "\n",
    "\n",
    "pair_time = np.zeros([40,2100])\n",
    "for i in range(40):\n",
    "    data = dataTrial1[:,i,:]\n",
    "    pair_time[i] = filter_time(data,ch1) - filter_time(data,ch2)\n",
    "\n",
    "\n",
    "pair_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_fs(dataTrial, trial, channel):\n",
    "    \"\"\" \n",
    "    eg: dataTrial1第一个人的数据。channel为处理的通道1-19,提取看到图片后的数据3-6s。滤波后重建。\n",
    "    result[0]为theta， [1]为beta\n",
    "    \"\"\"\n",
    "    data = dataTrial1[:,trial,:]  ##随便挑了一次trial,  900x19\n",
    "    n_row = 19\n",
    "    data_new = []\n",
    "    t = np.linspace(0,3,900)\n",
    "    for i in range(len(t)):\n",
    "        data_new.append([t[i],data[i,channel]])\n",
    "    data_new = np.array(data_new) ####### 900x2 ############\n",
    "    fig1 = pylab.rcParams['figure.figsize'] = (15.0,2.0)\n",
    "#     plt.plot(data_new[:,0],data_new[:,1])\n",
    "#     plt.show()\n",
    "    \n",
    "    b,a = signal.butter(5,[8/300,16/300],'bandpass')\n",
    "    theta = signal.filtfilt(b,a,data_new[:,1])\n",
    "#     plt.plot(t,theta)\n",
    "#     plt.title('theta')\n",
    "#     plt.show()\n",
    "    \n",
    "    b,a = signal.butter(5,[24/300,60/300],'bandpass')\n",
    "    beta = signal.filtfilt(b,a,data_new[:,1])\n",
    "#     plt.plot(t,beta)\n",
    "#     plt.title('beta')\n",
    "#     plt.show()\n",
    "    \n",
    "    result = [theta,beta]\n",
    "    \n",
    "    N = 900\n",
    "    Fs = 300\n",
    "    ds = Fs/N\n",
    "    yy = fft(theta)\n",
    "    yf = abs(yy[:int(N)])/N\n",
    "    yf = fftshift(yf) ##900\n",
    "    freq = np.arange(-N/2,N/2)*ds ###900\n",
    "#     plt.plot(freq, yf)\n",
    "#     plt.title('theta')\n",
    "#     plt.show()\n",
    "    \n",
    "    yy = fft(beta)\n",
    "    yf = abs(yy[:int(N)])/N\n",
    "    yf = fftshift(yf) ##900\n",
    "    freq = np.arange(-N/2,N/2)*ds ###900\n",
    "#     plt.plot(freq, yf)\n",
    "#     plt.title('beta')\n",
    "#     plt.show()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TSHOG\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\TSHOG\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\TSHOG\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(pair_time, label, test_size=0.2)\n",
    "#print(X_test1.shape)\n",
    "for i in range(40):\n",
    "    data = dataTrial1[:,i,:]\n",
    "#     pair_time[i] = filter_time(data,ch1) - filter_time(data,ch2)\n",
    "    X_train2[i] = cut_fs(dataTrial1, i, ch1)[0] - cut_fs(dataTrial1, i, ch2)[0]\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(2),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=100),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    DummyClassifier(strategy='most_frequent', random_state=0)]\n",
    "\n",
    "table = PrettyTable(['classifier','score'])\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train1, y_train1)\n",
    "    score = clf.score(X_test1, y_test1)\n",
    "    table.add_row([clf,score])\n",
    "#print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 40, 19)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrial1 = dataTrial1[900:1800,:,:]\n",
    "dataTrial1.shape  ###取3s以后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = cut_fs(dataTrial1, 7, 8)  ##for test\n",
    "result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# cut_fs(dataTrial, trial, channel)\n",
    "pair_time = np.zeros([40,54,900])\n",
    "for i in range(40):\n",
    "    for j in range(54):\n",
    "        data = dataTrial1[:,i,:]\n",
    "    #     pair_time[i] = filter_time(data,ch1) - filter_time(data,ch2)\n",
    "        pair_time[i][j] = cut_fs(dataTrial1, i, pair54[j][0])[0] - cut_fs(dataTrial1, i, pair54[j][1])[0]\n",
    "\n",
    "\n",
    "print(pair_time.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-1ee3b221444e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;31m#clf.fit(X_train1,y_train1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mscore2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m             X, y = self._validate_data(X, y, accept_sparse=\"csr\",\n\u001b[1;32m-> 1132\u001b[1;33m                                        multi_output=True)\n\u001b[0m\u001b[0;32m   1133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    801\u001b[0m                     \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    804\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[1;32m--> 642\u001b[1;33m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(pair_time, label, test_size=0.2)\n",
    "for i in range(10):\n",
    "    X_test2 = pair_time[pair_time.shape[0]/10*i:pair_time.shape[0]/10*(i+1),:,:]\n",
    "    X_train2 = pair_time[,:,:]\n",
    "    y_train2 = label[0:33]\n",
    "    y_test2 = label[33:]\n",
    "    classifiers = [\n",
    "       # LogisticRegression(),\n",
    "        KNeighborsClassifier(2),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "        MLPClassifier(alpha=1, max_iter=100),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        DummyClassifier(strategy='most_frequent', random_state=0)]\n",
    "\n",
    "    table = PrettyTable(['classifier','score2'])\n",
    "\n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train2, y_train2)\n",
    "        #clf.fit(X_train1,y_train1)\n",
    "        score2 = clf.score(X_test2, y_test2)\n",
    "        #score1 = clf.score(X_test1, y_test1)\n",
    "        table.add_row([clf,score2])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
