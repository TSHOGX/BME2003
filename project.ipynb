{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经信号的采集与人机智能技术概览课程作业——脑机接口数据分析\n",
    "<center>韩诗雯 范可茗 翁逸杰</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据预处理\n",
    "\n",
    "\n",
    "### 1. 原始数据的缺陷\n",
    "\n",
    "&emsp; 原始数据中，每个受试者共看了40张图片，在观察每张图片时，我们得到了19个通道在300Hz采样频率下获得的2100个数据点(共7s，每秒300个采样点)。直接进行分类会碰到曲线分类的通病——维度过高（此处有2100 samples $\\times$ 19 channels $\\times$ 40/10 trials 维度），因此要对其进行降维。同时，原始数据中的噪声很多，这就需要我们进行一定的处理，以获得原始数据中有价值的信息。\n",
    "\n",
    "\n",
    "\n",
    "### 2. 对原始数据的处理\n",
    "\n",
    "&emsp; 由于原始数据的维度高、噪声多，我们选择对其进行降维、滤波处理，具体如下：\n",
    "\n",
    "#### 1）降维处理\n",
    "&emsp; 首先，我们对原始信号进行截取，由于第三秒放出图片，且人类的反应时间一般在0.3s左右，因此我们取2.8s到3.8s。这也既保证了数据的完整性，也使得数据有了相当程度的降维（从2100到300）。\n",
    "\n",
    "&emsp; 接着，我们对截取的数据进行特征提取，即计算四个频段的相对频段能量，把能量作为输入特征，接着再利用PCA对19个通道进行降维，我们截取了前五个主成分，把19个通道降为5个。但其实对于这个数据集，我们尝试过300维曲线直接分类，在扩增（见下）之后的稳定性以及精确度已经不错了，所以我们并未使用降维降到能量(从300到4)的数据集。\n",
    "\n",
    "&emsp; 通过分析计算得出的能量以及PCA提取的第一主成分可以看出，不同人之间的看到图片后的大脑反映频段有所差异（代码见下）。在只有四个人的情况下，没有其余先天知识，我们很难模拟出一个适用性广泛的分类模型，所以在此只对一个人（以Person1为例）进行分类尝试。\n",
    "\n",
    "#### 2）降噪以及频段截取\n",
    "\n",
    "&emsp; 我们首先去除了比较明显的误差，如交流电、其他生物活动造成的生物电等。\n",
    "\n",
    "&emsp; 频段截取同时有降噪以及特征提取的功能。我们尝试了DWT的Daubechies（示意图如下），以及傅立叶变换（fft）滤出beta，theta等波段。对于波段上的特征选择，我们可以使用DWT中的多种滤波方式得到很多波段，再利用PCA找到最佳的主成分波段；也可以使用fft直接滤出常见的波段，然后利用PCA对通道进行主成分分析，找到能量占比最高的波段。根据代码输出结果，我们可以看到4-13Hz波段能量占比最大，这与查找的文献结果相符合，所以我们截取了在这其中的theta波作为最终的输入特征。\n",
    "\n",
    "\n",
    "#### 3）扩增数据集\n",
    "&emsp; 在使用降维与降噪之后的数据集作为输入时，我们遇到了训练结果不稳定的问题。因为原来的数据集只有40个样本，还需要分割数据集，导致训练样本只有30个。但是截取后的曲线有300个点，在这样的高维空间之中30个样本过于分散，使用任何算法得到的决策边界都会距离测试样本非常远，所以算法的置信度很高（利用sklearn的predict_proba计算得出），但是精确度很随机。\n",
    "\n",
    "&emsp; 因此，我们选择扩增数据集，即从2.7s到2.9s起，取61份1s内的数据，就是说对原有数据集进行时域上的平移。这样不仅简单地增加了样本的数量，还降低了人们在不同实验trial之中反应速度不同带来的误差。这样我们的训练样本从40扩展到了2440，并且实验得到的算法置信度有所降低，但是换来了非常稳定的精确度。\n",
    "\n",
    "&emsp; 之后我们还计算了PCA降维之后的算法稳定性，同样降低了一点点算法的置信度，不过2240个数据集的KNN算法精确度已经比较高了（某些通道在0.9+），所以降维之后的精确度没有很大变化，我们还是使用扩增之后的300维曲线数据进行训练。\n",
    "\n",
    "&emsp; 下面是我们相应的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.cm as cm\n",
    "import pylab\n",
    "import math\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.fftpack import rfft, irfft, fftfreq, fft, ifft, fftshift\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import torch\n",
    "\n",
    "import ipympl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pywt\n",
    "import pywt.data\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Channel Name\n",
    "\n",
    "通道的名称\n",
    "\n",
    "一共24个通道，rawTracePersonX之中只包含19个通道[1:8 10:16 19:20 23:24]，不包含9CM 17X3 18X2 21X1 22A2。cm（废弃通道），X1 X2 X3 A2为空通道（什么都没接）。\n",
    "\n",
    "<img src=\"./source/pic1.png\" width = \"40%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ChanName = loadmat('./input/ChanName.mat')\n",
    "chanName = np.array(ChanName['ChanName'])\n",
    "chanNamex = chanName.reshape(24,)\n",
    "\n",
    "chanName = []\n",
    "for i in range(24):\n",
    "    chanName.append(chanNamex[i][0][0])\n",
    "chanName = np.array(chanName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EEG Fp2 - Pz'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chanNameUsed = np.append(chanName[0:8], chanName[9:16])\n",
    "chanNameUsed = np.append(chanNameUsed, chanName[18:20])\n",
    "chanNameUsed = np.append(chanNameUsed, chanName[22:24])\n",
    "\n",
    "chanNameUsed[11] = 'EEG T7 - Pz'  ## T3 is now T7\n",
    "chanNameUsed[12] = 'EEG P7 - Pz'  ## T5 is now P7\n",
    "chanNameUsed[-2] = 'EEG P8 - Pz'  ## T6 is now P8\n",
    "chanNameUsed[-1] = 'EEG T8 - Pz'  ## T4 is now T8\n",
    "chanNameUsed[10]\n",
    "# 8 9 18 19 20 (21) 22 23 24 30 \n",
    "# 35 36 37 40 47 52 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# timeRawTrace = loadmat('./input/timeRawTrace.mat')\n",
    "# timeRawTrace = np.array(timeRawTrace['timeRawTrace'])\n",
    "# timeRawTrace = timeRawTrace.reshape(2100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "OSPerson1 = loadmat('./input/Person1/OSPerson1.mat')\n",
    "\n",
    "# os1 = OSPerson1['OS'] # OS：要分析的数据，36x52x40x54 time X freq X Trial x pair\n",
    "# time1 = OSPerson1['Time'] # why only 36?\n",
    "# freq1 = OSPerson1['fOS'] # fOS：对应的频率\n",
    "track1 = OSPerson1['Track'] # true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 18], dtype=uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通道：一共24个通道，此处只包含19个通道: [1:8 10:16 19:20 23:24] --> [1:8 9:15  16:17 18:19]\n",
    "pair54 = np.array(loadmat('./input/Pair54.mat')['Pair54'])\n",
    "for i in range(54):\n",
    "    for j in range(2):\n",
    "        if 10 <= pair54[i][j] <= 16:# & pair54[i][j] <= 16:\n",
    "            pair54[i][j] -= 2\n",
    "        elif 19 <= pair54[i][j] <= 20:\n",
    "            pair54[i][j] -= 4\n",
    "        elif 23 <= pair54[i][j] <= 24:\n",
    "            pair54[i][j] -= 6\n",
    "        else:\n",
    "            pair54[i][j] -= 1\n",
    "#     pass\n",
    "pair54[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# rawTracePersonX\n",
    "# dataTrail: 2100x40x19 时间 x Trial x 通道，包含了各通道各 Trail 的 Rawtrace\n",
    "# track: 40个trial对应的图片编号。<11的编号为记忆过的图片，>10的是没有记忆过的。顺序和 trail 对应\n",
    "rawTracePerson1 = loadmat('./input/Person1/rawTracePerson1.mat')\n",
    "track1 = np.array(rawTracePerson1['Track']).reshape(40,)\n",
    "dataTrial1 = np.array(rawTracePerson1['dataTrial'])\n",
    "\n",
    "rawTracePerson2 = loadmat('./input/Person2/rawTracePerson2.mat')\n",
    "track2 = np.array(rawTracePerson2['Track']).reshape(10,)\n",
    "dataTrial2 = np.array(rawTracePerson2['dataTrial'])\n",
    "\n",
    "rawTracePerson3 = loadmat('./input/Person3/rawTracePerson3.mat')\n",
    "track3 = np.array(rawTracePerson3['Track']).reshape(40,)\n",
    "dataTrial3 = np.array(rawTracePerson3['dataTrial'])\n",
    "\n",
    "rawTracePerson4 = loadmat('./input/Person4/rawTracePerson4.mat')\n",
    "track4 = np.array(rawTracePerson4['Track']).reshape(10,)\n",
    "dataTrial4 = np.array(rawTracePerson4['dataTrial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "&emsp; 首先，根据事实，我们将每次实验分为看到过图片和没有看到过两类。\n",
    "\n",
    "&emsp; 其次，提取2.8-3.8s的数据信息。\n",
    "\n",
    "&emsp; 最后，对原有数据进行时域上的平移(理由见上)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person 1 or 3\n",
    "\n",
    "# 数据结构：\n",
    "# data [data, pic, chanel] [300 x 2440 x 19]\n",
    "# pair [pic, pair, data] [2440 x 54 x 300]\n",
    "\n",
    "label_orgin = np.zeros([40])\n",
    "for i in range(40):\n",
    "    if track1[i] < 11:\n",
    "        label_orgin[i] = 0\n",
    "    if track1[i] > 10:\n",
    "        label_orgin[i] = 1\n",
    "\n",
    "label = label_orgin\n",
    "data = dataTrial1[840:1140,:,:] # 提取序列2.8-3.8秒\n",
    "\n",
    "# 平移扩增\n",
    "for i in range(-30,30):\n",
    "    label = np.append(label,label_orgin)\n",
    "    data = np.concatenate( (data, dataTrial1[840+i:1140+i,:,:]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA reduced data set\n",
    "\n",
    "&emsp; 首先我们尝试使用DWT对数据进行滤波处理。\n",
    "\n",
    "&emsp; 接下来，我们尝试滤波后计算各个频段的能量(power)占比，来对曲线进行特征提取。\n",
    "\n",
    "&emsp; 这部分还展示了原始波形的时域图。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于数据data[:,1,2], 尝试使用 DWT, Daubechies\n",
    "\n",
    "t = np.linspace(0,1,300)\n",
    "(cA, cD) = pywt.dwt(data[:,1,1], 'db1')\n",
    "tx = np.linspace(0,1,150)\n",
    "plt.plot(tx, cA, label='cA')\n",
    "plt.plot(tx, cD, label='cD')\n",
    "plt.plot(t, data[:,0,0], label='Orgin')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# related band power help function\n",
    "\n",
    "def power_cal(data, band, trial, channel):\n",
    "    y_data = data[:,trial, channel]\n",
    "\n",
    "def power_cal(data, band, trial, channel):\n",
    "    from scipy.integrate import simps\n",
    "    y_data = data[:,trial,channel]\n",
    "    low = band[0]\n",
    "    high = band[1]\n",
    "    #### 滤波\n",
    "    b,a = signal.butter(5,[2*low/300,2*high/300],'bandpass')\n",
    "    fil_sig = signal.filtfilt(b,a,data[:, trial, channel])\n",
    "    b,a = signal.butter(5,[0.01,2*80/300],'bandpass')\n",
    "    total_sig = signal.filtfilt(b,a,data[:, trial, channel])\n",
    "    #### 计算功率\n",
    "    sf = 300\n",
    "#     win = 4*sf\n",
    "    win = sf\n",
    "    freqs, psd = signal.welch(fil_sig, sf, nperseg=win)\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "    band_power = simps(psd[idx_band], dx=freqs[1]-freqs[0])\n",
    "\n",
    "    freqs, psd = signal.welch(total_sig, sf, nperseg=win)\n",
    "    idx_total = np.logical_and(freqs >= 0, freqs <= 80)\n",
    "    total_power = simps(psd[idx_total], dx=freqs[1]-freqs[0])\n",
    "    \n",
    "    result = band_power/total_power\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各个频段的 related bandpower\n",
    "\n",
    "# 数据结构：\n",
    "# related_bandpower[sub-band x chanel x pic]\n",
    "\n",
    "DELTA = [1,3]\n",
    "THETA = [4,8]\n",
    "ALPHA = [9,13]\n",
    "BETA = [14,30]\n",
    "sub_band = [DELTA, THETA, ALPHA, BETA]\n",
    "pic = 2440 # 之后如果加入四个人可以修改\n",
    "related_bandpower = np.zeros([len(sub_band), 19, pic])\n",
    "for ch in range(19):\n",
    "    for p in range(pic):\n",
    "        for b in range(len(sub_band)):\n",
    "            related_bandpower[b,ch,p] = power_cal(data, sub_band[b], p, ch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization of each sub-band: zero mean and unit variance\n",
    "\n",
    "scaled_related_bandpower = np.zeros([len(sub_band), 19, pic])\n",
    "for b in range(len(sub_band)):\n",
    "    for c in range(19):\n",
    "        scaled_related_bandpower[b,:,:] = preprocessing.scale(related_bandpower[b,c,:])\n",
    "#         transformer = preprocessing.RobustScaler().fit(related_bandpower[b,:,:])\n",
    "#         scaled_related_bandpower[b,:,:] = transformer.transform(related_bandpower[b,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2440, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA 降维: fix band, reduce chanel\n",
    "\n",
    "# 数据结构：\n",
    "# reduced [band x pic x chanel] (4, 2440, 5)\n",
    "\n",
    "reduced = np.zeros([4,2440,5])\n",
    "\n",
    "for band in range(4):\n",
    "    temp = scaled_related_bandpower[band,:,:]\n",
    "    t = np.zeros([2440,19])\n",
    "    for i in range(2240):\n",
    "        for j in range(19):\n",
    "            t[i][j] = temp[j][i]\n",
    "\n",
    "    randomized_pca = PCA(n_components=5, svd_solver='randomized')\n",
    "    reduced[band,:,:] = randomized_pca.fit_transform(t)\n",
    "\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察不同人的反映程度\n",
    "\n",
    "&emsp; 对于任何一张图片，4个人的四个band所展示的反映激烈程度可能会不一样，我们选择具有40轮实验的1号和3号人员，以及他们的PCA的第一主成分进行反映激烈程度的展示（下列结果是标准化之后的）：\n",
    "\n",
    "&emsp; 结果可以发现，两个人都在theta和alpha波段有明显的能量，所以对这个实验影响最大的波段应该是theta和alpha，就是4-13Hz。\n",
    "\n",
    "&emsp; 但是两者的能量分布有着比较明显的差异，在实际算法使用中还是选择比较小的波段范围，效果会更好（4-13Hz对于高维少数据量的样本来说还是太宽了），所以我们只选择一个人进行有针对性的训练。\n",
    "\n",
    "&emsp; 并且可以看出，第三个人比第一个人具有更高的反映程度，实际训练结果也显示出第一个人的正确率较高，虽然只有0.1（展示在算法部分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.79557798e-16,  1.74723624e-16,  2.62085435e-16, -8.15376910e-17])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# std = True; person = 1; 四个类型波段能量占比\n",
    "\n",
    "# reduced1 = reduced # run all above cell and choose one to comment \n",
    "\n",
    "power_mean1 = np.zeros([4,])\n",
    "for band in range(4):\n",
    "    power_mean1[band] = np.mean(reduced1[band,:,0])\n",
    "power_mean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.82412078e-18,  0.00000000e+00,  8.15376910e-17,  1.04834174e-16])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# std = True; person = 3; 四个类型波段能量占比\n",
    "\n",
    "# reduced3 = reduced # run all above cell and choose one to comment \n",
    "\n",
    "power_mean3 = np.zeros([4,])\n",
    "for band in range(4):\n",
    "    power_mean3[band] = np.mean(reduced3[band,:,0])\n",
    "power_mean3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and filter: \n",
    "\n",
    "&emsp; 这部分主要展示了原始数据以及滤波之后的数据；并且一部分作为help function在接下来的代码中用到。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_time(data,i):\n",
    "    '''\n",
    "    filter and output time domain y\n",
    "\n",
    "    data: dataTrial1[:,n_trail,:] # (2100,19) one trail's brain wave\n",
    "    i: data[:,i] # channel we want to see\n",
    "    '''\n",
    "    f = np.zeros((2100, 2), dtype=float)\n",
    "    f[:,1] = data[:,i]\n",
    "    f[:,0] = np.arange(2100)/300\n",
    "\n",
    "    N = 2100 \n",
    "    T = 7.0 / 2100.0\n",
    "    x = f[:,0]\n",
    "    y = f[:,1]\n",
    "\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "\n",
    "    f_signal = rfft(y)\n",
    "    W = fftfreq(y.size, d=x[1]-x[0])\n",
    "\n",
    "    cut_f_signal = f_signal.copy()\n",
    "    cut_f_signal[(W<1)] = 0 # filter all frequencies below 1\n",
    "\n",
    "    cut_signal = irfft(cut_f_signal)\n",
    "    \n",
    "    return cut_signal\n",
    "\n",
    "\n",
    "def filter_freq(data,i):\n",
    "    '''\n",
    "    filter and output freq domain y\n",
    "\n",
    "    data: dataTrial1[:,n_trail,:] # (2100,19) one trail's brain wave\n",
    "    i: data[:,i] # channel we want to see\n",
    "    '''\n",
    "    \n",
    "    f = np.zeros((2100, 2), dtype=float)\n",
    "    f[:,1] = data[:,i]\n",
    "    f[:,0] = np.arange(2100)/300\n",
    "\n",
    "    N = 2100 \n",
    "    T = 7.0 / 2100.0 \n",
    "    x = f[:,0]\n",
    "    y = f[:,1]\n",
    "\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "\n",
    "    f_signal = rfft(y)\n",
    "    W = fftfreq(y.size, d=x[1]-x[0])\n",
    "\n",
    "    cut_f_signal = f_signal.copy()\n",
    "    cut_f_signal[(W<1)] = 0  # filter all frequencies below 1\n",
    "\n",
    "    cut_signal = irfft(cut_f_signal)\n",
    "\n",
    "    cut_signal_f = fft(cut_signal)\n",
    "\n",
    "    return cut_signal_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_freq: plot_freq(track1_remember[1])\n",
    "def plot_freq(n_trail,fl): \n",
    "    '''\n",
    "    randomly choose from 40 trails\n",
    "    plot frequency domain of 19 channel\n",
    "    '''\n",
    "    size = 5\n",
    "    fig = plt.figure(figsize=(size,2*size))\n",
    "    data = dataTrial1[:,n_trail,:] # (2100,19)\n",
    "#     data = np.delete(data,7,1) # delete one row\n",
    "    n_rows = 19\n",
    "    n_samples = 2100\n",
    "\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_xlim(0, 7) # x's range: 0-7s\n",
    "    ax.set_xticks(np.arange(7))\n",
    "    dmin = 0\n",
    "    dmax = 20\n",
    "    dr = (dmax - dmin)\n",
    "    y0 = dmin\n",
    "    y1 = (n_rows - 1) * dr + dmax\n",
    "    ax.set_ylim(y0, y1)\n",
    "\n",
    "    N = 2100 \n",
    "    T = 7.0 / 2100.0\n",
    "    tf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    \n",
    "    segs = [] # list of lines, each line is an array of points\n",
    "    for i in range(n_rows):\n",
    "        if fl:\n",
    "            yf = 2.0/N * np.abs(filter_freq(data,i)[:N//2])\n",
    "            segs.append(np.column_stack((tf, yf)))  ### filter\n",
    "        else:\n",
    "            yf = 2.0/N * np.abs(data[:,i][:N//2])\n",
    "            segs.append(np.column_stack((tf, yf)))\n",
    "            \n",
    "\n",
    "    offsets = np.zeros((n_rows, 2), dtype=float)\n",
    "    offsets[:, 1] = np.arange(n_rows) * dr\n",
    "\n",
    "    lines = LineCollection(segs, offsets=offsets, transOffset=None) # draw lines from segs\n",
    "    ax.add_collection(lines)\n",
    "\n",
    "    ax.set_yticks(np.arange(n_rows) * dr)\n",
    "    ax.set_yticklabels(chanNameUsed)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# plot_time: plot_freq(track1_not_remember[1])\n",
    "def plot_time(n_trail,fl): \n",
    "    '''\n",
    "    randomly choose from 40 trails\n",
    "    plot time domain of 19 channel\n",
    "    '''\n",
    "    \n",
    "    size = 10\n",
    "    fig = plt.figure(figsize=(size,size))\n",
    "    n_rows = 19 # delete one row\n",
    "    n_samples = 2100\n",
    "    data = dataTrial1[:,n_trail,:] # (2100,19)\n",
    "#     data = np.delete(data,7,1)\n",
    "    t = np.arange(2100) / 300\n",
    "\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_xlim(0, 7.01) # x's range: 0-7s\n",
    "    ax.set_xticks(np.arange(7))\n",
    "    dmin = data.min()\n",
    "    dmax = data.max()\n",
    "    dr = (dmax - dmin)\n",
    "    y0 = dmin\n",
    "    y1 = (n_rows - 1) * dr + dmax\n",
    "    ax.set_ylim(y0, y1)\n",
    "\n",
    "    segs = [] # list of lines, each line is an array of points\n",
    "    for i in range(n_rows):\n",
    "        if fl:\n",
    "            segs.append(np.column_stack((t, filter_time(data,i))))  ### filter\n",
    "        else:\n",
    "            segs.append(np.column_stack((t, data[:,i])))\n",
    "\n",
    "    offsets = np.zeros((n_rows, 2), dtype=float)\n",
    "    offsets[:, 1] = np.arange(n_rows) * dr\n",
    "\n",
    "    lines = LineCollection(segs, offsets=offsets, transOffset=None) # draw lines from segs\n",
    "    ax.add_collection(lines)\n",
    "\n",
    "    ax.set_yticks(np.arange(n_rows) * dr)\n",
    "    ax.set_yticklabels(chanNameUsed)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b249466deb254dd3bbefd240ff956fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3c4143746e442fbe630393eaf67cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_time(1,False)\n",
    "plot_time(1,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; 这部分是滤波的具体代码。之前通过查阅相关资料，我们首先锁定了theta和beta两个波段，所以函数cut的也是这两个；后面根据这个数据集算出对实验影响最大的是theta波段，所以最后启用的是theta波\n",
    "\n",
    "&emsp; 首先，我们对原始信号进行傅立叶变换fft。\n",
    "\n",
    "&emsp; 得到频谱图后，提取需要的相关频率(theta：4-8Hz，beta：14-30Hz）得到theta和beta波的频谱图。\n",
    "\n",
    "&emsp; 重建信号，得到theta和beta波的时域图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_fs(data):\n",
    "    '''\n",
    "    data: (300,)\n",
    "    result[0]: theta, [1]: beta, (300,)\n",
    "    '''\n",
    "    b,a = signal.butter(5,[8/300,16/300],'bandpass')\n",
    "    theta = signal.filtfilt(b,a,data)\n",
    "    b,a = signal.butter(5,[24/300,60/300],'bandpass')\n",
    "    beta = signal.filtfilt(b,a,data)\n",
    "    result = [theta,beta]\n",
    "    return result\n",
    "\n",
    "\n",
    "def cut_fss(dataTrial, trial, channel):\n",
    "    \"\"\" \n",
    "    Input:  dataTrial channel 为处理的通道1-19\n",
    "    Output: 提取看到图片后的数据3-6s。滤波后重建。\n",
    "            result[0]: theta, [1]: beta\n",
    "            并且画图\n",
    "    \"\"\"\n",
    "    data = dataTrial[:,trial,:]  ##随便挑了一次trial,  900x19\n",
    "    n_row = 19\n",
    "    data_new = []\n",
    "    t = np.linspace(0,3,900)\n",
    "    for i in range(len(t)):\n",
    "        data_new.append([t[i],data[i,channel]])\n",
    "    data_new = np.array(data_new) ####### 900x2 ############\n",
    "    fig1 = pylab.rcParams['figure.figsize'] = (15.0,2.0)\n",
    "#     plt.plot(data_new[:,0],data_new[:,1])\n",
    "#     plt.show()\n",
    "    \n",
    "    b,a = signal.butter(5,[8/300,16/300],'bandpass')\n",
    "    theta = signal.filtfilt(b,a,data_new[:,1])\n",
    "#     plt.plot(t,theta)\n",
    "#     plt.title('theta')\n",
    "#     plt.show()\n",
    "    \n",
    "    b,a = signal.butter(5,[24/300,60/300],'bandpass')\n",
    "    beta = signal.filtfilt(b,a,data_new[:,1])\n",
    "#     plt.plot(t,beta)\n",
    "#     plt.title('beta')\n",
    "#     plt.show()\n",
    "    \n",
    "    result = [theta,beta]\n",
    "    \n",
    "    N = 900\n",
    "    Fs = 300\n",
    "    ds = Fs/N\n",
    "    yy = fft(theta)\n",
    "    yf = abs(yy[:int(N)])/N\n",
    "    yf = fftshift(yf) ##900\n",
    "    freq = np.arange(-N/2,N/2)*ds ###900\n",
    "#     plt.plot(freq, yf)\n",
    "#     plt.title('theta')\n",
    "#     plt.show()\n",
    "    \n",
    "    yy = fft(beta)\n",
    "    yf = abs(yy[:int(N)])/N\n",
    "    yf = fftshift(yf) ##900\n",
    "    freq = np.arange(-N/2,N/2)*ds ###900\n",
    "#     plt.plot(freq, yf)\n",
    "#     plt.title('beta')\n",
    "#     plt.show()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 制作新的数据结构 (pair)\n",
    "\n",
    "&emsp; 此外，从生物学角度出发，从19个通道中选取了部分可能相关的通道，两两排列组合构成了54个pair。\n",
    "\n",
    "&emsp; 我们还尝试用不同的pair代替原有的19个channel作为新的输入，放到网络中训练。我们的pair也采用滤波后的theta波段作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据结构：\n",
    "# pair[pic, pair, data] 2440 x 54 x 300\n",
    "\n",
    "# 与之前的label、data对应注释这格或者下一格\n",
    "\n",
    "pair1 = np.zeros([2440,54,300])\n",
    "for i in range(2440):\n",
    "    for j in range(54):\n",
    "        pair1[i][j] = cut_fs(data[:,i,pair54[j][0]])[0] - cut_fs(data[:,i,pair54[j][1]])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair3 = np.zeros([2440,54,300])\n",
    "for i in range(2440):\n",
    "    for j in range(54):\n",
    "        pair3[i][j] = cut_fs(data[:,i,pair54[j][0]])[0] - cut_fs(data[:,i,pair54[j][1]])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、分类算法\n",
    "\n",
    "&emsp; 在进行过数据预处理后，我们选用了10种算法对其进行分类，详细参数和内容如下列表所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "#     LogisticRegression(),\n",
    "#     KNeighborsClassifier(2),\n",
    "#     SVC(gamma=2, C=1, probability=True),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "#     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1, max_iter=100),\n",
    "#     AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "#     QuadraticDiscriminantAnalysis(),\n",
    "#     DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&emsp; 在扩展数据集之后，KNN性价比最高（稳定而且运行速度快），所以最后选择KNN作为分析算法。\n",
    "\n",
    "\n",
    "### 正确率计算：\n",
    "\n",
    "&emsp; 测试两个人在扩展数据集之后的正确率。\n",
    "\n",
    "&emsp; 根据预处理阶段的分析，我们预测第一个人的正确率应该更高，这与运行结果相符合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2440个数据，KNN算法运行结果\n",
    "\n",
    "models = pd.DataFrame(columns = classifiers)\n",
    "test_score = np.zeros([54,])\n",
    "test_probs = pd.DataFrame(columns = range(54))\n",
    "\n",
    "for clf in classifiers:\n",
    "    for p in range(54):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(pair3[:,p,:], label, test_size=0.25)\n",
    "        clf.fit(X_train, y_train)\n",
    "        test_score[p] = clf.score(X_test, y_test)\n",
    "        test_probs[p] = clf.predict_proba(X_test)[:,0]\n",
    "    models[clf] = test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNeighborsClassifier(n_neighbors=2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.967213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.962295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    KNeighborsClassifier(n_neighbors=2)\n",
       "18                             0.967213\n",
       "19                             0.962295"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# person1 18/19 pair 的正确率\n",
    "# models1 = models\n",
    "\n",
    "models1[18:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNeighborsClassifier(n_neighbors=2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.808197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.837705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    KNeighborsClassifier(n_neighbors=2)\n",
       "18                             0.808197\n",
       "19                             0.837705"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# person3 54 pair 的正确率\n",
    "models3 = models\n",
    "\n",
    "models3[18:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试算法的置信程度：\n",
    "\n",
    "&emsp; 与预测结果相符合——用40个数据求出的算法的置信度最高，而且波动也比较大；用2240个数据求出的算法置信度降低了一点；用PCA降维之后的2240个数据稳定在0.01左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008396766832840203"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 40张的原始数据循环运行，输出pair18和19的正确率，观察稳定性\n",
    "\n",
    "test_probs = pd.DataFrame(columns = range(54))\n",
    "\n",
    "for clf in classifiers:\n",
    "    for p in range(54):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(pair1[0:40,p,:], label[0:40], test_size=0.25)\n",
    "        clf.fit(X_train, y_train)\n",
    "        test_probs[p] = clf.predict_proba(X_test)[:,0]\n",
    "test_probs.mean(axis=1).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0037403122926294974"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2440个数据循环运行，输出pair18和19的正确率，观察稳定性\n",
    "test_probs = pd.DataFrame(columns = range(54))\n",
    "\n",
    "for clf in classifiers:\n",
    "    for p in range(54):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(pair1[:,p,:], label, test_size=0.25)\n",
    "        clf.fit(X_train, y_train)\n",
    "        test_probs[p] = clf.predict_proba(X_test)[:,0]\n",
    "test_probs.mean(axis=1).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016519589864122431"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用PCA降维之后的 reduced [band x pic x chanel] (4, 2440, 5) \n",
    "\n",
    "test_probs = pd.DataFrame(columns = range(4))\n",
    "\n",
    "for clf in classifiers:\n",
    "    for b in range(4):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reduced[b,:,:], label, test_size=0.25)\n",
    "        clf.fit(X_train, y_train)\n",
    "        test_probs[b] = clf.predict_proba(X_test)[:,0]\n",
    "test_probs.mean(axis=1).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稳定性测试：\n",
    "\n",
    "&emsp; 分别循环运行以2440和40个数据作为输入的KNN算法，观察score的变化，可以发现40张图片的算法波动很大而且误差较高，而扩增后的2440数据集则相对稳定得多，而且有更高的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.944262\n",
      "19                             0.944262\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.945902\n",
      "19                             0.967213\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.940984\n",
      "19                             0.959016\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.970492\n",
      "19                             0.955738\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.952459\n",
      "19                             0.959016\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.945902\n",
      "19                             0.952459\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.945902\n",
      "19                             0.945902\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.957377\n",
      "19                             0.949180\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.955738\n",
      "19                             0.963934\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                             0.957377\n",
      "19                             0.981967\n"
     ]
    }
   ],
   "source": [
    "# 2440个数据循环运行，输出pair18和19的正确率，观察稳定性\n",
    "\n",
    "models = pd.DataFrame(columns = classifiers)\n",
    "test_score = np.zeros([54,])\n",
    "\n",
    "for i in range(10):\n",
    "    for clf in classifiers:\n",
    "        for p in range(54):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(pair1[:,p,:], label, test_size=0.25)\n",
    "            clf.fit(X_train, y_train)\n",
    "            test_score[p] = clf.score(X_test, y_test)\n",
    "        models[clf] = test_score\n",
    "    print(models[18:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.2\n",
      "19                                  0.4\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.4\n",
      "19                                  0.4\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.2\n",
      "19                                  0.5\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.3\n",
      "19                                  0.4\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.4\n",
      "19                                  0.5\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.6\n",
      "19                                  0.6\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.4\n",
      "19                                  0.4\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.6\n",
      "19                                  0.3\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.3\n",
      "19                                  0.7\n",
      "    KNeighborsClassifier(n_neighbors=2)\n",
      "18                                  0.5\n",
      "19                                  0.5\n"
     ]
    }
   ],
   "source": [
    "# 40张的原始数据循环运行，输出pair18和19的正确率，观察稳定性\n",
    "\n",
    "models = pd.DataFrame(columns = classifiers)\n",
    "test_score = np.zeros([54,])\n",
    "test_probs = pd.DataFrame(columns = range(54))\n",
    "\n",
    "for i in range(10):\n",
    "    for clf in classifiers:\n",
    "        for p in range(54):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(pair1[0:40,p,:], label[0:40], test_size=0.25)\n",
    "            clf.fit(X_train, y_train)\n",
    "            test_score[p] = clf.score(X_test, y_test)\n",
    "            test_probs[p] = clf.predict_proba(X_test)[:,0]\n",
    "        models[clf] = test_score\n",
    "    print(models[18:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、结论与展望\n",
    "\n",
    "&emsp; 在对原始数据进行预处理并分类后，我们发现，可以从原始的脑电信号中用适当的算法以较高的准确率判断出受试者是否记住了测试图像。其中，我们也发现预处理数据手法的不同对分类算法精度的影响是非常大的。因此，我们可以展望，在未来，只要以更加科学、精确的方式对脑电数据进行较好的预处理，我们便可以从较为粗糙的数据中得到非常实用的信息，这可以降低硬件的成本，为家庭医疗设施的普及做出较大的贡献。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
